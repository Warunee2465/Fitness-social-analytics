# -*- coding: utf-8 -*-
"""Social_fitness_insights_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cDx5MsaaG18T5IbszfHP8kyYsIIP-b3c

---


# **1. Data Analysis and Presentation Preparation**


---



### **Topic: Social Fitness Insights**

---
- We combine a rule-based tag engine with NMF/LDA topic modeling because rules give business reliable categories, while topics discover hidden themes together they‚Äôre accurate and exploratory for better decisions.





---

### **Method summary.**

---

- We first clean and standardize the social posts, then apply a rule-based tag engine to map each post into our 8 business categories (e.g., Workout, Facilities, Service) because it‚Äôs deterministic, explainable, and aligns to owner goals.

- In parallel, we run NMF/LDA topic modeling because it reveals organic clusters and emerging themes we didn‚Äôt predefine.
- We compare both to validate categories, quantify coverage/agreement, and surface insights that translate into campaign and ops actions.

## üå¥ **Step 1: Data Preparation and Cleaning (Question 1.4 & Key Responsibility)**


---
---

Data prep and  cleaning standardize columns, remove duplicates, normalize text, parse dates, and safely handle missing engagement because reliable features prevent biased tagging and unstable topics.

## **#0. Set up library and Import data**


---
"""

!pip install pythainlp

!apt-get install fonts-thai-tlwg

!pip install -q pythainlp scikit-learn pyLDAvis

# Core
import os
import re
import unicodedata
from itertools import islice
from dataclasses import dataclass, field
from typing import List, Pattern, Dict

# Data & Progress
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# NLP (Thai language processing)
from pythainlp import word_tokenize
from pythainlp.corpus.common import thai_stopwords

# Machine Learning / Topic Modeling
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.metrics.pairwise import cosine_similarity

# Load the Excel file
file_path = "/content/Test_Data_Analyst__-_Spacebar.xlsx"
# Read only the "Data" sheet
social_df = pd.read_excel(file_path, sheet_name="Data")

# Save cleaned version (only data sheet)
social_df.to_csv("fitness_data_clean.csv", index=False)

# Display the column names and their data types
social_df.info()

"""## **1) Understand the Data and analysis (EDA)**

---

### **a. Basic exploration**
"""

# 1. Head / Tails

display(social_df.head())
display(social_df.tail())

# 2. Shape and dtypes

print("Shape", social_df.shape)

social_df.info()

# 3. Summary statistics and Transpose for readability

social_df.describe().T

"""**Why so many NaNs?**

---



Looking at our dataset, we noticed several missing or irrelevant columns:

- Tags : completely empty ‚Üí we need to create this ourselves

- Notes : completely empty ‚Üí can be dropped

- Age : only 2 values (20 yrs old) ‚Üí not useful

- City, Country, Bio, Gender : partially missing (common in social media datasets)

- Interactions : some missing (likely because not every post had recorded engagement)

Therefore, not all columns are useful. We‚Äôll keep only those that matter for insights:

["id", "Title", "Detail", "Publish date", "Sentiment", "Media type", "Reach", "Interactions", "Gender", "City", "Country"]

## **2) Data Preprocesing**

---

### **a. Check missing values**

---
"""

# 1. Missing values
social_df.isna().sum()

# 2. Check for duplicate rows
duplicate = social_df.duplicated().sum()
print(f"Number of duplicate: {duplicate}")

"""###**Missing Value Analysis**

- Completely Empty (all missing)

    - Tags (4382) ‚Üí you need to create this.

    - Age (4380 missing, only 2 values) ‚Üí not useful ‚Üí drop.

    - Notes (4382 missing) ‚Üí drop.

- Partially Missing

    - Interactions (199 missing) ‚Üí safe to fill with 0 (means no engagement recorded).

    - Country (2579 missing) ‚Üí can fill with "Unknown".

    - Gender (972 missing) ‚Üí fill "Unknown".

    - Bio (1783 missing) ‚Üí optional, but can fill "Unknown".

    - City (3499 missing, only ~20% filled) ‚Üí keep, but missing = "Unknown".

    - Author name, Author handle, Author URL (200‚Äì250 missing) ‚Üí keep, missing = "Unknown".

- No Missing

    - id, Title, Detail, Link, Source, Update date, Publish date, Sentiment, Ranking, Media type, Audience, Reach are all complete .

### **b. Cleans data after checking Nan and misng values**

---
"""

# 1. # Drop useless columns
social_df = social_df.drop(columns=["Notes", "Age"], errors="ignore")

print(type(social_df))

# 2.Check which columns are left
social_df.columns

# 3.Confirm missing values again - After dropping columns, check if there are still NaN values:
social_df.isnull().sum()

# 4. Standardize column names
social_df.columns = (social_df.columns.str.strip()
                        .str.lower()
                        .str.replace(" ", "_")
                        .str.replace("__", "_"))

# 5. Trim whitespace on text columns
for c in social_df.select_dtypes(include=["object"]).columns:
    social_df[c] = social_df[c].str.strip()

# 6. Define helper function + detect column names

def find_col(social_df, candidates):
    cols_lower = {c.lower(): c for c in social_df.columns}
    for cand in candidates:
        if cand.lower() in cols_lower:
            return cols_lower[cand.lower()]
    for c in social_df.columns:
        for cand in candidates:
            if cand.lower() in c.lower():
                return c
    return None

col_publish_date = find_col(social_df, ["Publish date","Published","publish_date","Date","‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏û‡∏™‡∏ï‡πå"])
col_update_date  = find_col(social_df, ["update_date", "Update date", "Updated", "Update"])
col_reach        = find_col(social_df, ["reach", "Reach"])
col_audience     = find_col(social_df, ["audience", "Audience"])
col_interactions = find_col(social_df, ["interactions", "Interactions"])
col_country      = find_col(social_df, ["country", "Country"])
col_gender       = find_col(social_df, ["gender", "Gender"])
col_bio          = find_col(social_df, ["bio", "Bio"])
col_city         = find_col(social_df, ["city", "City"])
col_media        = find_col(social_df, ["media_type", "Media type"])
col_sentiment    = find_col(social_df, ["sentiment", "Sentiment"])
col_title  = find_col(social_df, ["Title","Post","Content","Caption","Message","Text","‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"])
col_detail = find_col(social_df, ["Detail","Description","Body","‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"])
col_author_name  = find_col(social_df, ["author_name", "Author name"])
col_author_handle= find_col(social_df, ["author_handle_(@username)", "author_handle", "@username"])
col_author_url   = find_col(social_df, ["author_url", "Author URL"])

# 7. Drop duplicates (conservative)
subset_keys = [k for k in [col_title, col_detail, col_publish_date] if k]
social_df = social_df.drop_duplicates(subset=subset_keys or None, keep="first")

# 8. Parse dates robustly (dayfirst and errors='coerce'), using detected columns
if col_update_date:
    social_df[col_update_date]  = pd.to_datetime(social_df[col_update_date],  errors="coerce", dayfirst=True)
if col_publish_date:
    social_df[col_publish_date] = pd.to_datetime(social_df[col_publish_date], errors="coerce", dayfirst=True)

# 9. Standardize categoricals
if col_sentiment:
    social_df[col_sentiment] = social_df[col_sentiment].str.strip().str.title()   # Positive/Negative/Neutral
if col_media:
    social_df[col_media] = social_df[col_media].str.strip().str.lower()           # twitter, instagram, etc.

# 10. Coerce numerics; add missingness flags BEFORE imputation
for c in [col_reach, col_audience, col_interactions]:
    if c:
        social_df[c] = pd.to_numeric(social_df[c], errors="coerce")
social_df["interactions_missing"] = social_df[col_interactions].isna().astype(int) if col_interactions else 0

# 11. Conservative impute for Interactions (median within media x sentiment), otherwise overall median
if col_interactions:
    if col_media and col_sentiment:
        grp_med = social_df.groupby([col_media, col_sentiment])[col_interactions].transform("median")
        social_df[col_interactions] = social_df[col_interactions].fillna(grp_med)
    social_df[col_interactions] = social_df[col_interactions].fillna(social_df[col_interactions].median())

# 12. Fill non-sensitive text fields with 'Unknown' but DO NOT infer identities
for c, fill in [
    (col_country, "Unknown"),
    (col_city,    "Unknown"),
    (col_gender,  "Unknown"),
    (col_bio,     "Unknown"),
    (col_author_name,  "Unknown"),
    (col_author_handle,"Unknown"),
    (col_author_url,   "Unknown"),
]:
    if c:
        social_df[c] =  social_df[c].fillna(fill).replace({"": fill})

# 13. Time features for posting-time charts (safe if publish_date exists)
if col_publish_date:
    social_df["dow"]  = social_df[col_publish_date].dt.day_name()
    social_df["hour"] = social_df[col_publish_date].dt.hour
    social_df["month"]= social_df[col_publish_date].dt.to_period("M").astype(str)

# 14. Final sanity checks
print("Duplicates removed:", len(social_df) - len(social_df))
print(social_df.dtypes)
print(social_df.isna().sum().sort_values(ascending=False))

print(social_df.columns)

"""- Next Step:

With a clean dataset, we can now define tags based on keywords, sentiment, or other relevant criteria.

# üå¥**Step 2: Tagging and Categorization (Question 1.2 & Key Responsibility)**

---

**Goal:** Create a new column (e.g. tags or category) that classifies each social media post into meaningful business categories inorder to analyze trends (e.g. "Most negative comments are about Equipment").

## **Method 1):  Rule-based categorization**:


---


- A transparent keyword engine maps posts to 8 fitness categories (Workout/Training, Nutrition, Equipment & Facilities, Promotion & Events, Illness/Health Risks, Motivation & Mental Health, Customer Service & Feedback, Influencer/Celeb) because business stakeholders need deterministic, explainable buckets tied to decisions.

### **a. Regex/tag engine with explainability**

---
"""

# =========================
# 1) TagRule engine
# =========================

# --- Helpers: normalization ---
# Always work on a copy to avoid SettingWithCopyWarning

# --- imports once ---
import re, unicodedata
import pandas as pd
from dataclasses import dataclass, field
from typing import List

social_df = social_df.copy()

# --------- helpers ----------
ZERO_WIDTH_RE = re.compile(r"[\u200b\u200c\u200d\ufeff]")
URL_PAT    = re.compile(r"https?://\S+|www\.\S+")
HANDLE_PAT = re.compile(r"@\w+")
EMOJI_RE   = re.compile(r"[\U00010000-\U0010ffff]", flags=re.UNICODE)
ELONG_RE   = re.compile(r"(.)\1{2,}")
thai_noise = {"‡∏õ‡πà‡∏∞","‡∏£‡πä","‡∏≠‡πä‡∏∞","‡πÄ‡∏≠‡πà‡∏≠","‡∏Ñ‡πà‡∏∞","‡∏Ñ‡∏∞","‡∏Ñ‡∏£‡∏±‡∏ö","555","‡∏Æ‡πà‡∏≤","‡∏à‡πâ‡∏≤","‡∏ô‡πâ‡∏≤","‡∏°‡∏±‡πâ‡∏Å‡∏Å‡∏Å‡∏Å","‡∏Å‡∏Å‡∏Å‡∏Å","‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ","‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤"}

def find_col(df, candidates):
    cols_lower = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in cols_lower:
            return cols_lower[cand.lower()]
    for c in df.columns:
        for cand in candidates:
            if cand.lower() in c.lower():
                return c
    return None

col_title  = find_col(social_df, ["Title","Post","Content","Caption","Message","Text","‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"])
col_detail = find_col(social_df, ["Detail","Description","Body","‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"])


def combine_text(row: pd.Series) -> str:
    parts = []
    if col_title  and pd.notna(row.get(col_title)):   parts.append(str(row[col_title]))
    if col_detail and pd.notna(row.get(col_detail)): parts.append(str(row[col_detail]))
    return " ".join(parts) if parts else ""

# --- 1) text_raw: light combine only (NO heavy cleaning here) ---
social_df["text_raw"] = social_df.apply(combine_text, axis=1)

# Fallback if combine found nothing
if (social_df["text_raw"].fillna("") == "").all():
    fallback = find_col(social_df, ["text","content","post","caption","message"])
    if fallback:
        social_df["text_raw"] = social_df[fallback].astype(str)

# --- 2) text_for_tag: stripping for rules (urls/handles/emojis/elongation etc.) ---
def normalize_text(s: str) -> str:
    s = "" if s is None else str(s)
    s = unicodedata.normalize("NFKC", s)
    s = ZERO_WIDTH_RE.sub("", s)
    s = s.replace("\u0e4d\u0e32","\u0e33")  # ‡πç+‡∏≤ -> ‡∏≥
    s = URL_PAT.sub(" ", s)
    s = HANDLE_PAT.sub(" ", s)
    s = EMOJI_RE.sub(" ", s)
    s = ELONG_RE.sub(r"\1\1", s)
    for w in thai_noise:
        s = s.replace(w, " ")
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s

social_df["text_for_tag"] = social_df["text_raw"].apply(normalize_text)

# --- 3) text_clean: ML-ready (can be same as tag or slightly lighter) ---
def clean_for_ml(s: str) -> str:
    s = "" if s is None else str(s)
    s = unicodedata.normalize("NFKC", s)
    s = ZERO_WIDTH_RE.sub("", s)
    s = s.replace("\u0e4d\u0e32","\u0e33")
    s = URL_PAT.sub(" ", s)
    s = HANDLE_PAT.sub(" ", s)
    # Notice: we KEEP emojis, elongation, slang for ML variation
    return re.sub(r"\s+", " ", s).strip().lower()

social_df["text_clean"] = social_df["text_raw"].apply(clean_for_ml)


# --- 4) drop near-empty AFTER we have text_clean ---
social_df = social_df[social_df["text_clean"].str.len() >= 5].reset_index(drop=True)

# --- 1) build text_for_tag and text_clean FROM text_raw ---
url_pat    = re.compile(r"https?://\S+|www\.\S+")
handle_pat = re.compile(r"@\w+")
social_df["text_for_tag"] = (
    social_df["text_raw"]
        .str.replace(url_pat, " ", regex=True)
        .str.replace(handle_pat, " ", regex=True)
        .apply(normalize_text)
)
social_df["text_clean"] = social_df["text_for_tag"]  # reuse for LDA/NMF

# --- 2) TagRule engine (unchanged) ---
def compile_kw(words, word_boundary=True):
    pats = []
    for w in words:
        if not w: continue
        if word_boundary and re.fullmatch(r"[A-Za-z0-9_]+", w):
            pats.append(re.compile(rf"\b{re.escape(w)}\b", flags=re.IGNORECASE))
        else:
            pats.append(re.compile(re.escape(w), flags=re.IGNORECASE))
    return pats

@dataclass
class TagRule:
    name: str
    include_words: List[str]
    exclude_words: List[str] = field(default_factory=list)
    min_hits: int = 1
    def compile(self):
        self._inc = compile_kw(self.include_words, word_boundary=True)
        self._exc = compile_kw(self.exclude_words, word_boundary=True)
        return self
    def match(self, text: str):
        inc_hits = [p.pattern for p in self._inc if p.search(text)]
        exc_hits = [p.pattern for p in self._exc if p.search(text)]
        ok = (len(inc_hits) >= self.min_hits) and (len(exc_hits) == 0)
        confidence = min(1.0, len(inc_hits) / max(1, self.min_hits))
        return ok, inc_hits, exc_hits, confidence


# 8 categories with Thai variants observed in your samples
RULES = [
    TagRule("Workout / Training",
            ["gym","exercise","workout","training","run","cardio","strength","yoga","weight","lift",
             "‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á","‡∏≠‡∏≠‡∏Å‡∏Å‡πç‡∏≤‡∏•‡∏±‡∏á","‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢","‡∏≠‡∏≠‡∏Å‡∏Å‡πç‡∏≤‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢","‡∏ü‡∏¥‡∏ï‡πÄ‡∏ô‡∏™","‡πÄ‡∏ß‡∏ó","‡πÇ‡∏¢‡∏Ñ‡∏∞","‡∏ß‡∏¥‡πà‡∏á","‡∏Ñ‡∏≤‡∏£‡πå‡∏î‡∏¥‡πÇ‡∏≠","‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå","‡∏Ñ‡∏•‡∏≤‡∏™","‡∏¢‡∏¥‡∏°",
             "‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢","‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡∏Å‡πç‡∏≤‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢"],
            ["‡∏ö‡πâ‡∏≤‡∏ô‡∏û‡∏±‡∏Å","‡∏´‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏Å","outfit"]).compile(),

    TagRule("Nutrition / Diet",
            ["diet","protein","meal","nutrition","calorie","supplement","weightloss","keto","clean",
             "‡∏≠‡∏≤‡∏´‡∏≤‡∏£","‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å","‡πÇ‡∏õ‡∏£‡∏ï‡∏µ‡∏ô","‡∏Ñ‡∏µ‡πÇ‡∏ï","‡∏Ñ‡∏•‡∏µ‡∏ô","‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£","‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°","‡πÄ‡∏°‡∏ô‡∏π","‡πÅ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ"],
            []).compile(),

    TagRule("Equipment & Facilities",
            ["equipment","machine","treadmill","facility","locker","shower","clean","room","studio",
             "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á","‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå","‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà","‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥","‡∏•‡πá‡∏≠‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå","‡∏ü‡∏¥‡∏ï‡πÄ‡∏ô‡∏™","‡∏≠‡∏≤‡∏ö‡∏ô‡πâ‡∏≥","‡∏´‡πâ‡∏≠‡∏á","‡∏•‡∏π‡πà‡∏ß‡∏¥‡πà‡∏á"],
            []).compile(),

    TagRule("Promotion & Events",
            ["discount","promo","promotion","sale","offer","campaign","package","membership","join",
             "‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô","‡∏•‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤","‡∏™‡∏°‡∏±‡∏Ñ‡∏£","‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à","‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å","‡πÅ‡∏Ñ‡∏°‡πÄ‡∏õ‡∏ç","‡∏£‡∏≤‡∏Ñ‡∏≤","‡∏ö‡∏≤‡∏ó","‡∏Ñ‡∏≠‡∏£‡πå‡∏™"],
            []).compile(),

    TagRule("Illness / Health Risks",
            ["injury","fatigue","illness","chronic","sick","pain","symptom","risk","fever","cold","cough",
             "‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á","‡πÇ‡∏£‡∏Ñ","‡∏≠‡∏≤‡∏Å‡∏≤‡∏£","‡∏õ‡πà‡∏ß‡∏¢","‡πÄ‡∏£‡∏∑‡πâ‡∏≠‡∏£‡∏±‡∏á","‡∏†‡∏≤‡∏ß‡∏∞","‡πÑ‡∏Ç‡πâ","‡∏´‡∏ß‡∏±‡∏î","‡πÄ‡∏à‡πá‡∏ö","‡∏ö‡∏≤‡∏î‡πÄ‡∏à‡πá‡∏ö",
             "‡∏≠‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏•‡∏µ‡∏¢","‡∏á‡πà‡∏ß‡∏á","‡∏á‡πà‡∏ß‡∏á‡∏ô‡∏≠‡∏ô","‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏°‡∏µ‡πÅ‡∏£‡∏á","‡∏ô‡∏≠‡∏ô‡πÑ‡∏°‡πà‡∏´‡∏•‡∏±‡∏ö","‡∏†‡∏π‡∏°‡∏¥‡πÅ‡∏û‡πâ"],
            ["‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô","‡πÇ‡∏õ‡∏£","‡∏•‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤"]).compile(),

    TagRule("Motivation & Mental Health",
            ["motivation","mental","mindset","happy","inspiration","progress","stress","confidence","focus",
             "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à","‡πÅ‡∏£‡∏á‡∏ö‡∏±‡∏ô‡∏î‡∏≤‡∏•‡πÉ‡∏à","‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï","‡∏™‡∏π‡πâ‡πÜ","‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢","‡∏û‡∏±‡∏í‡∏ô‡∏≤","‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô","‡∏î‡∏µ‡∏°‡∏≤‡∏Å","‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à"],
            []).compile(),

    TagRule("Customer Service & Feedback",
            ["complaint","feedback","service","staff","booking","payment","cancel","problem","support",
             "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô","‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£","‡∏à‡∏≠‡∏á","‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô","‡∏ä‡∏≥‡∏£‡∏∞","‡∏õ‡∏±‡∏ç‡∏´‡∏≤","‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠","‡πÇ‡∏ó‡∏£","‡πÅ‡∏à‡πâ‡∏á"],
            []).compile(),

    TagRule("Influencer / Celebrity Mentions",
            ["celebrity","influencer","idol","coach","trainer","famous","follow","youtube","tiktok","ig",
             "‡∏î‡∏≤‡∏£‡∏≤","‡πÄ‡∏ã‡πÄ‡∏•‡∏ö","‡πÇ‡∏Ñ‡πâ‡∏ä","‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå","‡∏¢‡∏π‡∏ó‡∏π‡∏õ","‡∏ï‡∏¥‡πä‡∏Å‡∏ï‡πä‡∏≠‡∏Å","‡∏≠‡∏¥‡∏ô‡∏™‡∏ï‡∏≤‡πÅ‡∏Å‡∏£‡∏°","‡πÑ‡∏≠‡∏î‡∏≠‡∏•"],
            []).compile(),
]

def extract_tags(text: str, verbose=False):
    if not isinstance(text, str) or not text.strip():
        return ["Uncategorized"], {}, 0.0
    tags, matched, scores = [], {}, []
    for rule in RULES:
        ok, inc_hits, exc_hits, conf = rule.match(text)
        if ok:
            tags.append(rule.name)
            matched[rule.name] = {"include_hits": inc_hits, "exclude_hits": exc_hits}
            scores.append(conf)
    if not tags:
        return ["Uncategorized"], {}, 0.0
    return sorted(set(tags)), matched, float(sum(scores)/len(scores))

print(social_df.columns.tolist())
print(social_df[["text_raw","text_clean"]].head(3))

"""### **b. Text building and cleaning**"""

# =========================
# 2) Preview & full tagging
# =========================

for i, row in social_df.sample(5, random_state=42).iterrows():
    tags, matched, conf = extract_tags(row.get("text_for_tag",""))
    print(f"[Row {i}] Tags={tags}  Conf={conf:.2f}")

tags_list, matched_list, conf_list = [], [], []
for _, row in tqdm(social_df.iterrows(), total=len(social_df)):
    tags, matched, conf = extract_tags(row.get("text_for_tag",""))
    tags_list.append("; ".join(tags))
    matched_list.append(matched)
    conf_list.append(conf)

social_df["tags"]            = tags_list
social_df["tags_matched"]    = matched_list
social_df["tags_confidence"] = conf_list
social_df["tags_source"]     = np.where(social_df["tags"].eq("Uncategorized"), "auto-none", "auto-rule")
social_df["primary_tag"]     = social_df["tags"].str.split(";").str[0]
for tag in social_df["primary_tag"].unique():

    row = social_df[social_df["primary_tag"] == tag].sample(1, random_state=42)
    print(f"Example for {tag}:")
    print(row[["text_raw","tags","tags_confidence"]].to_string(index=False))
    print("-"*50)

coverage = (social_df["tags"] != "Uncategorized").mean()
print(f"Coverage: {coverage:.1%} of posts tagged")

cats = [
 "Workout / Training","Nutrition / Diet","Equipment & Facilities",
 "Promotion & Events","Illness / Health Risks","Motivation & Mental Health",
 "Customer Service & Feedback","Influencer / Celebrity Mentions","Uncategorized"
]
for cat in cats:
    ex = social_df[social_df["tags"].str.contains(cat, na=False)].head(3)
    print(f"\n=== {cat} ({len(ex)} shown / {social_df['tags'].str.contains(cat, na=False).sum()} total) ===")
    for _, r in ex.iterrows():
        print("-", r["text_raw"][:120])

# Quick summaries (any-tag vs primary-tag)
all_hits = (social_df["tags"].str.get_dummies(sep="; ").sum().sort_values(ascending=False))
primary_hits = social_df["primary_tag"].value_counts()

summary = pd.DataFrame({"any_tag_hits": all_hits, "primary_hits": primary_hits}) \
            .fillna(0).astype(int).sort_values("any_tag_hits", ascending=False)
print(summary)

summary_plot = summary.copy()
summary_plot.index.name = "Category"

# Create bar chart
ax = summary_plot.plot(
    kind="bar",
    figsize=(12,6),
    width=0.8,
    color=["skyblue", "tomato"]
)

plt.title("Post Tagging: Any-Tag vs Primary-Tag Counts", fontsize=14, weight="bold")
plt.xlabel("Category", fontsize=12)
plt.ylabel("Number of Posts", fontsize=12)
plt.xticks(rotation=45, ha="right")
plt.legend(["Any Tag Hits", "Primary Tag Hits"], fontsize=10)
plt.tight_layout()
plt.show()

"""**This split gives us insight:**
- any_tag_hits = overall conversation volume around a topic.
- primary_hits = focus level (when users choose that as the main subject).
"""

# Find all posts where ‚ÄúWorkout‚Äù was tagged but not primary

mask = social_df["tags"].str.contains("Workout / Training") & (social_df["primary_tag"] != "Workout / Training")
sample_overlap = social_df[mask][["text_raw","tags","primary_tag"]].head(20)
sample_overlap

"""- **Count which categories ‚Äústeal‚Äù the primary tag from Workout:**"""

overlap_counts = (social_df[mask]["primary_tag"]
                  .value_counts()
                  .sort_values(ascending=False))
print(overlap_counts)

"""## **Method 2) : Topic modeling (NMF/LDA)**
- discover data-driven clusters and top terms because it surfaces organic conversations and emerging themes our rules may miss.
"""

# ----- tokenizer for Thai+Eng -----
thai_stops = set(thai_stopwords())
eng_stops  = {"the","a","an","and","or","of","to","for","in","on","at","is","are","be",
              "it","this","that","with","by","as","from","about","rt","via","amp","i",
              "you","we","they","he","she","them","me","my","your","our"}

def tokenize_bilingual(s:str):
    toks = word_tokenize(s, engine="newmm")
    out = []
    for t in toks:
        t = t.strip().lower()
        if len(t) <= 1: continue
        if t in thai_stops or t in eng_stops: continue
        if re.fullmatch(r"\W+", t): continue
        out.append(t)
    return out

# ----- TF-IDF -----
tfidf = TfidfVectorizer(
    tokenizer=tokenize_bilingual,
    ngram_range=(1,2),
    min_df=5,
    max_df=0.95,
    max_features=30000,
    sublinear_tf=True,
)
X = tfidf.fit_transform(social_df["text_clean"])
feat = tfidf.get_feature_names_out()
print("X shape (docs √ó terms):", X.shape)

# ----- NMF (k=10) -----
nmf = NMF(
    n_components=10,
    init="nndsvda",
    random_state=42,
    solver="mu",
    beta_loss="kullback-leibler",
    max_iter=1000,
    alpha_W=0.1, alpha_H=0.1,
    l1_ratio=0.5
)
W = nmf.fit_transform(X)
H = nmf.components_
print("W shape (docs √ó topics):", W.shape)
print("H shape (topics √ó terms):", H.shape)

# topic distinctiveness (lower max ~ better)
sim = cosine_similarity(H); np.fill_diagonal(sim, 0)
print("Max topic similarity:", sim.max())

# ----- human labels: top terms per topic -----
def top_terms(H, feat, topn=12):
    labels = []
    for row in H:
        idx = row.argsort()[-topn:]
        labels.append(", ".join(feat[i] for i in idx))
    return labels

topic_terms_nmf = top_terms(H, feat, topn=12)
for i, terms in enumerate(topic_terms_nmf):
    print(f"Topic {i}: {terms}")

# ----- per-doc assignment + normalized strength -----
topic_id = W.argmax(axis=1)
row_sums = W.sum(axis=1, keepdims=True)
W_norm = W / np.where(row_sums==0, 1, row_sums)
topic_strength = W_norm.max(axis=1)

social_df["topic_id_nmf"]   = topic_id
social_df["topic_prob_nmf"] = topic_strength
social_df["topic_label_nmf"]= pd.Series(topic_id).map(dict(enumerate(topic_terms_nmf)))

# 8 business categories
rule_cats = [
   "Workout / Training","Nutrition / Diet","Equipment & Facilities",
   "Promotion & Events","Illness / Health Risks","Motivation & Mental Health",
   "Customer Service & Feedback","Influencer / Celebrity Mentions"
]

# Keyword hints per category (can expand over time)
cat_kw = {
 "Workout / Training": {"‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢","‡∏ß‡∏¥‡πà‡∏á","‡πÇ‡∏¢‡∏Ñ‡∏∞","‡∏Ñ‡∏≤‡∏£‡πå‡∏î‡∏¥‡πÇ‡∏≠","‡∏ü‡∏¥‡∏ï‡πÄ‡∏ô‡∏™","‡∏¢‡∏¥‡∏°","‡πÄ‡∏ß‡∏ó","treadmill","workout","run","‡πÄ‡∏î‡∏¥‡∏ô","‡∏ä‡∏±‡∏ô","45 ‡∏ô‡∏≤‡∏ó‡∏µ"},
 "Nutrition / Diet": {"‡πÇ‡∏õ‡∏£‡∏ï‡∏µ‡∏ô","‡∏≠‡∏≤‡∏´‡∏≤‡∏£","‡∏Å‡∏¥‡∏ô","‡∏ó‡∏≤‡∏ô","‡∏Ñ‡∏µ‡πÇ‡∏ï","‡∏Ñ‡∏•‡∏µ‡∏ô","‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£","‡πÅ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ","supplement","‡∏ä‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß","‡∏°‡∏±‡∏ó‡∏â‡∏∞","‡∏û‡∏∑‡∏ä"},
 "Equipment & Facilities": {"‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå","‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á","‡∏•‡∏π‡πà‡∏ß‡∏¥‡πà‡∏á","‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥","‡∏•‡πá‡∏≠‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå","‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà","‡∏™‡∏ï‡∏π‡∏î‡∏¥‡πÇ‡∏≠","‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢","treadmill","locker","shower"},
 "Promotion & Events": {"‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô","‡∏•‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤","‡∏™‡∏°‡∏±‡∏Ñ‡∏£","‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à","‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å","‡∏£‡∏≤‡∏Ñ‡∏≤","‡∏Ñ‡∏≠‡∏£‡πå‡∏™","‡πÇ‡∏õ‡∏£","‡πÅ‡∏Ñ‡∏°‡πÄ‡∏õ‡∏ç","sale","discount"},
 "Illness / Health Risks": {"‡πÇ‡∏£‡∏Ñ","‡∏õ‡πà‡∏ß‡∏¢","‡∏≠‡∏≤‡∏Å‡∏≤‡∏£","‡πÄ‡∏à‡πá‡∏ö","‡∏ö‡∏≤‡∏î‡πÄ‡∏à‡πá‡∏ö","‡πÄ‡∏£‡∏∑‡πâ‡∏≠‡∏£‡∏±‡∏á","‡πÑ‡∏Ç‡πâ","‡∏´‡∏ß‡∏±‡∏î","‡∏†‡∏π‡∏°‡∏¥‡πÅ‡∏û‡πâ","‡∏≠‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏•‡∏µ‡∏¢","‡∏ô‡∏≠‡∏ô‡πÑ‡∏°‡πà‡∏´‡∏•‡∏±‡∏ö","‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"},
 "Motivation & Mental Health": {"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à","‡πÅ‡∏£‡∏á‡∏ö‡∏±‡∏ô‡∏î‡∏≤‡∏•‡πÉ‡∏à","‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï","‡∏™‡∏π‡πâ‡πÜ","‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢","‡∏û‡∏±‡∏í‡∏ô‡∏≤","‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô","‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à","mindset","motivation"},
 "Customer Service & Feedback": {"‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô","‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£","‡∏à‡∏≠‡∏á","‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô","‡∏ä‡∏≥‡∏£‡∏∞","‡∏õ‡∏±‡∏ç‡∏´‡∏≤","‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠","‡πÇ‡∏ó‡∏£","‡πÅ‡∏à‡πâ‡∏á","staff","service","booking","refund"},
 "Influencer / Celebrity Mentions": {"‡∏î‡∏≤‡∏£‡∏≤","‡πÄ‡∏ã‡πÄ‡∏•‡∏ö","‡πÑ‡∏≠‡∏î‡∏≠‡∏•","‡∏¢‡∏π‡∏ó‡∏π‡∏õ","‡∏ï‡∏¥‡πä‡∏Å‡∏ï‡πä‡∏≠‡∏Å","‡∏≠‡∏¥‡∏ô‡∏™‡∏ï‡∏≤‡πÅ‡∏Å‡∏£‡∏°","influencer","coach","trainer","ig","tiktok","youtube"}
}

noise = {"‡∏õ‡πà‡∏∞","‡∏£‡πä","vibes","‡∏û‡∏£‡∏´‡∏°‡∏•‡∏¥‡∏Ç‡∏¥‡∏ï","‡∏Å‡∏≤‡∏£‡πå‡∏î","‡πÅ‡∏ü‡∏ô‡∏Ñ‡∏•‡∏±‡∏ö","‡πÄ‡∏™‡∏∑‡πâ‡∏≠","tee","‡πÄ‡∏ö‡∏ç‡∏à"}

def map_topic_terms_to_cat(terms_str: str) -> str:
    if not isinstance(terms_str, str) or not terms_str.strip():
        return "Other"
    # remove obvious noise tokens
    terms = [t.strip() for t in terms_str.split(",")]
    terms = [t for t in terms if t and all(n not in t for n in noise)]
    if not terms: return "Other"
    # fuzzy substring scoring
    scores = {c:0 for c in rule_cats}
    for c, kws in cat_kw.items():
        for term in terms:
            if any(kw in term or term in kw for kw in kws):
                scores[c] += 1
    best = max(scores, key=scores.get)
    return best if scores[best] > 0 else "Other"

topic_to_cat = {i: map_topic_terms_to_cat(t) for i, t in enumerate(topic_terms_nmf)}
social_df["focus_from_nmf"] = social_df["topic_id_nmf"].map(topic_to_cat)
pd.DataFrame({"topic": list(topic_to_cat.keys()),
              "top_words": topic_terms_nmf,
              "mapped_cat": [topic_to_cat[i] for i in range(len(topic_terms_nmf))]})

"""## **3) Comparison/validation**

---



- measure coverage and agreement between methods; investigate mismatches because agreement builds confidence and disagreements point to new rules or trends.
"""

import matplotlib
matplotlib.set_loglevel("error")

ct = pd.crosstab(social_df["primary_tag"], social_df["focus_from_nmf"]).reindex(index=rule_cats, columns=rule_cats+["Other"], fill_value=0)

plt.figure(figsize=(12,6))
sns.heatmap(ct, annot=True, fmt="d", cmap="Blues")
plt.title("Rule-based Primary Tag vs NMF (k=10) ‚Üí 8-Category Mapping", fontsize=14)
plt.ylabel("Rule-based (primary_tag)")
plt.xlabel("NMF mapped category (focus_from_nmf)")
plt.tight_layout()
plt.show()
mask_valid = social_df["focus_from_nmf"].isin(rule_cats)
agree = (social_df.loc[mask_valid, "primary_tag"] == social_df.loc[mask_valid, "focus_from_nmf"]).mean()
print(f"Agreement on mapped categories (excluding 'Other'): {agree:.1%}")

# Coverage by method
rule_cov = (social_df["primary_tag"] != "Uncategorized").mean()
nmf_cov  = mask_valid.mean()
print(f"Rule-based coverage: {rule_cov:.1%} | NMF mapped coverage (non-Other): {nmf_cov:.1%}")

from itertools import combinations
from collections import Counter

pairs = []
for tags in social_df["tags"]:
    if not isinstance(tags,str): continue
    tag_list = tags.split("; ")
    for a,b in combinations(sorted(set(tag_list)),2):
        pairs.append((a,b))

pair_counts = Counter(pairs).most_common(15)
pd.DataFrame(pair_counts, columns=["Tag Pair","Count"]).plot(
    x="Tag Pair", y="Count", kind="barh", figsize=(10,6), legend=False, color="steelblue")
plt.title("Most Common Category Overlaps")
plt.show()

"""- We used two methods: a rule-based keyword engine for reliable tagging, and NMF topic modeling to discover latent themes. The overlap (Nutrition) confirms our categories, while mismatches (Workout, Customer Service) highlight areas where unsupervised methods need tuning."""

rule_cov = (social_df["primary_tag"] != "Uncategorized").mean()
nmf_cov  = social_df["focus_from_nmf"].ne("Other").mean()
agree = (social_df.loc[social_df["focus_from_nmf"].ne("Other"), "primary_tag"]
         == social_df.loc[social_df["focus_from_nmf"].ne("Other"), "focus_from_nmf"]).mean()
print(f"Rule-based coverage: {rule_cov:.1%}")
print(f"NMF mapped coverage (non-Other): {nmf_cov:.1%}")
print(f"Agreement (where NMF ‚â† Other): {agree:.1%}")

"""## üå¥ **Step 3: Analysis, Visualization, and Summarization (Question 1.1)**

---

### **1. Create Visualizations**
We use our cleaned and tagged data to generate insights.  
We focus on **5‚Äì7 high-impact visuals**, each paired with a short takeaway:

- **Where do customers talk the most?** ‚Üí training, facilities, staff.  
- **Are they happy or unhappy?** ‚Üí sentiment split (positive vs negative).  
- **When do they talk?** ‚Üí peak times/days for promotions.  
- **What problems do they raise?** ‚Üí injury, service, overcrowding, equipment.   

---

### **2. Extract Insights & Write Summaries**
For each visualization, we provide a clear conclusion that a gym owner can act on.

---


‚úÖ At this point, we have three data sources: raw data, rule-based categories, and NMF clusters. We decided to proceed mainly with NMF because it achieved the highest accuracy. However, for certain attributes where grouping is not required and data is complete (e.g., gender, city), we continue to use the raw data directly.

---
"""

print(social_df.columns.tolist())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
sns.set_theme(style="whitegrid")

import matplotlib.pyplot as plt
from matplotlib import rcParams
import numpy as np
import pandas as pd
import pathlib, os, urllib.request, zipfile
import matplotlib.font_manager as fm

# Install a Thai-compatible font using apt-get
!apt-get install -y fonts-thai-tlwg

# Rebuild matplotlib font cache
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Laksaman']

# Attempt to find the font file after installation
font_files = fm.findSystemFonts(fontpaths=["/usr/share/fonts/thai-tlwg/"], fontext="ttf")

if len(font_files) > 0:
    font_path = font_files[0] # Use the first found font file
    font_prop = fm.FontProperties(fname=font_path)
    rcParams['font.family'] = font_prop.get_name()
    print(f"Using font: {font_prop.get_name()} from {font_path}")
else:
    print("Laksaman font not found after installation. Using default.")

# Dashboard look: dark bg, clean text, slightly larger fonts
rcParams.update({
    "figure.facecolor": "#0F172A",
    "axes.facecolor":  "#0F172A",
    "axes.edgecolor":  "#1F2937",
    "axes.labelcolor": "#E5E7EB",
    "text.color":      "#E5E7EB",
    "xtick.color":     "#D1D5DB",
    "ytick.color":     "#D1D5DB",
    "grid.color":      "#334155",
    "axes.grid": True,
    "grid.linewidth": 0.6,
    "figure.dpi": 140,
    "axes.titleweight": "bold",
    "axes.titlepad": 12,
})

def _annotate_barh(ax):
    for p in ax.patches:
        w = p.get_width()
        y = p.get_y() + p.get_height()/2
        ax.text(w + max(ax.get_xlim()[1]*0.01, 3), y, f"{int(w):,}", va="center")

"""## **Slide 1: Conversation Volume by Category**
- Where do customers talk the most? (training, facilities, staff, etc.)

  - **Chart:** Bar chart of volume by rule-based categories (Training, Nutrition, Facilities, Customer Service, Promotions).

  - **Why:** Shows what customers care about most : helps prioritize service improvements & marketing.
---


"""

def plot_volume_by_category(df, col="primary_tag", topn=None, title="Most discussed topics"):
    counts = df[col].value_counts()
    if topn: counts = counts.head(topn)
    fig, ax = plt.subplots(figsize=(9,6))
    ax.barh(counts.index[::-1], counts.values[::-1], height=0.6)
    # Use font_prop if it was successfully created in the font setup cell
    if 'font_prop' in globals():
        ax.set_title(title, fontsize=16, fontproperties=font_prop)
        ax.set_xlabel("Post", fontproperties=font_prop)
        ax.set_ylabel("Category", fontproperties=font_prop)
        ax.tick_params(axis='both', labelsize=10)
    else:
        ax.set_title(title, fontsize=16)
        ax.set_xlabel("Post")
        ax.set_ylabel("Category")
        ax.tick_params(axis='both', labelsize=10)
    ax.grid(axis="x", linestyle="--", alpha=0.5)
    _annotate_barh(ax)
    plt.tight_layout()
    plt.show()
plot_volume_by_category(social_df)

"""## **Slide 2: Overall Sentiment Distribution**

- Are they happy or unhappy? (positive vs negative posts)

    - Chart: Stacked bar (Positive/Negative/Neutral within each category).

    - Why: Owner can see if Facilities or Service are driving complaints, while Motivation or Training drive positivity.
---


"""

# Donut using original sentiment data
if col_sentiment:
    # Count all sentiments in original data
    s_counts = social_df[col_sentiment].fillna("Unspecified").value_counts()

    # Donut chart
    plt.figure(figsize=(5,5))
    wedges, texts = plt.pie(
        s_counts.values,
        labels=None,
        autopct=None,
        startangle=90
    )

    # donut hole
    circle = plt.Circle((0,0), 0.70, fc="white")
    fig = plt.gcf()
    fig.gca().add_artist(circle)

    plt.title("Overall Sentiment Breakdown")
    plt.legend(wedges, s_counts.index, loc="center left", bbox_to_anchor=(1, 0.5))
    plt.tight_layout()
    plt.show()

    display(s_counts.rename_axis("Sentiment").to_frame("Count"))

else:
    print("No Sentiment column found.")

"""# **Slide 3: Sentiment by Category**
- What problems do they complain about? (injury, service, overcrowding, equipment)


---


"""

ct = pd.crosstab(social_df["primary_tag"], social_df["sentiment"])
ct = ct[["Positive","Neutral","Negative"]]  # order

ct.plot(kind="bar", stacked=True, figsize=(10,6),
        color=["lightgreen","gold","tomato"], edgecolor="black")

plt.title("Sentiment Breakdown by Category", fontsize=14, weight="bold")
plt.ylabel("Number of Posts")
plt.xlabel("Category")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

"""# **Slide 4: Engagement Over Time (by Hour)**

- When do they talk? (best times/days for promotions)

  - Chart: Heatmap (Day √ó Hour of posts).

  - Why: Best timing for promotions and new program launches.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging, matplotlib

matplotlib.set_loglevel("error")
logging.getLogger('matplotlib.font_manager').setLevel(logging.CRITICAL)

# 1) Set columns
col_publish_date = "publish_date"
col_id = "id" if "id" in social_df.columns else "__row_id__"

# 2) Parse datetime
social_df[col_publish_date] = pd.to_datetime(social_df[col_publish_date], errors="coerce")
df_time = social_df.dropna(subset=[col_publish_date]).copy()

# 3) Ensure day-of-week & hour exist
if "dow" in df_time.columns and "hour" in df_time.columns:
    # normalize to ints (0=Mon..6=Sun) and hours 0..23
    df_time["dow"]  = pd.to_numeric(df_time["dow"], errors="coerce").fillna(
        df_time[col_publish_date].dt.dayofweek
    ).astype(int).clip(0,6)
    df_time["hour"] = pd.to_numeric(df_time["hour"], errors="coerce").fillna(
        df_time[col_publish_date].dt.hour
    ).astype(int).clip(0,23)
else:
    df_time["dow"]  = df_time[col_publish_date].dt.dayofweek  # 0=Mon
    df_time["hour"] = df_time[col_publish_date].dt.hour       # 0..23

dow_order = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]

# 4) Line ‚Äî daily volume
daily = df_time.groupby(df_time[col_publish_date].dt.date).size()
if len(daily) > 0:
    plt.figure(figsize=(10,4))
    plt.plot(daily.index, daily.values)
    plt.title("Mentions Over Time")
    plt.xlabel("Date")
    plt.ylabel("Mentions")
    plt.tight_layout()
    plt.show()

# 5) Bar ‚Äî Day of Week
day_counts = (df_time[col_publish_date].dt.day_name()
              .value_counts()
              .reindex(dow_order, fill_value=0))
plt.figure(figsize=(8,4))
plt.bar(day_counts.index, day_counts.values)
plt.title("Mentions by Day of Week")
plt.xlabel("Day")
plt.ylabel("Mentions")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging

import matplotlib
matplotlib.set_loglevel("error")
logging.getLogger('matplotlib.font_manager').setLevel(logging.CRITICAL)

def find_col(df, candidates):
    """Return the actual column name in df that matches any candidate (case-insensitive, fuzzy contains)."""
    # exact case-insensitive match
    lower_map = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in lower_map:
            return lower_map[cand.lower()]
    # fuzzy "contains"
    for c in df.columns:
        for cand in candidates:
            if cand.lower() in c.lower():
                return c
    return None

# 1) Locate important columns
col_publish_date = find_col(social_df, ["publish date","publish_date","published","date","created at","created"])
col_id           = find_col(social_df, ["id","post id","post_id","identifier"])

# Fallback if id missing
if col_id is None:
    social_df = social_df.copy()
    social_df["__row_id__"] = range(1, len(social_df)+1)
    col_id = "__row_id__"

# Guard: ensure we found a usable datetime column
if col_publish_date is None:
    raise ValueError("Could not find a publish date column. Please check your column names.")

# 2) Convert to datetime safely
social_df[col_publish_date] = pd.to_datetime(social_df[col_publish_date], errors="coerce")

# Drop rows without a valid timestamp
df_time = social_df.dropna(subset=[col_publish_date]).copy()

if df_time.empty:
    print("No usable publish date values after parsing. Time-based charts cannot be built.")
else:
    # 3) Extract day & hour
    df_time["day_of_week"] = df_time[col_publish_date].dt.day_name()
    df_time["hour_of_day"] = df_time[col_publish_date].dt.hour

    # 4) Counts by day
    ordered_days = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
    day_counts = (df_time["day_of_week"]
                  .value_counts()
                  .reindex(ordered_days, fill_value=0))

    # 5) Counts by hour
    hour_counts = df_time["hour_of_day"].value_counts().sort_index()

    # 6) Plot: posts by day
    plt.figure(figsize=(10,4))
    day_counts.plot(kind="bar")
    plt.title("Posts by Day of Week")
    plt.ylabel("Number of Posts")
    plt.xlabel("Day")
    plt.tight_layout()
    plt.show()

    # 7) Plot: posts by hour
    plt.figure(figsize=(10,4))
    hour_counts.plot(kind="bar")
    plt.title("Posts by Hour of Day")
    plt.ylabel("Number of Posts")
    plt.xlabel("Hour (0‚Äì23)")
    plt.tight_layout()
    plt.show()


    # 9) Quick text takeaway you can paste to slides
    top_day = day_counts.idxmax()
    top_hour = hour_counts.idxmax()
    print(f"Peak day: {top_day} | Peak hour: {top_hour}:00")

"""## **Slide 5: Customer Demographics**

  - Chart: Pie or bar for Gender / City breakdown.

  - Why: Helps understand audience profile ‚Üí tailor campaigns (e.g., more yoga classes if women dominate).
---


"""

# Gender distribution
if col_gender:
    social_df[col_gender].value_counts().plot(kind="pie", autopct='%1.1f%%')
    plt.title("Gender Distribution")
    plt.ylabel("")
    plt.show()

"""## **Slide 6: Engagement by Media Type**

  - Chart: Bar chart comparing interactions across Video, Image, Text posts.

  - Why: Reveals which content type drives the most engagement : guide social media strategy..
---


"""

# City distribution (Top 10)
if 'city' in social_df.columns: # Check if 'city' column exists
    city_counts = social_df["city"].value_counts()
    if len(city_counts) > 0:
        plt.figure(figsize=(10,5))
        city_counts.head(10).plot(kind="bar")
        plt.title("Top 10 Cities Distribution")
        plt.xlabel("City")
        plt.ylabel("Count")
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        plt.show()

import matplotlib.pyplot as plt
if "media_type" in social_df.columns and "interactions" in social_df.columns:
    social_df["interactions"] = pd.to_numeric(social_df["interactions"], errors="coerce")
    media_engagement = social_df.groupby("media_type")["interactions"].sum().sort_values(ascending=False)

    plt.figure(figsize=(8,4))
    media_engagement.plot(kind="bar", color="skyblue")
    plt.title("Engagement by Media Type")
    plt.ylabel("Total Interactions")
    plt.xlabel("Media Type")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

    display(media_engagement.to_frame("Total Interactions"))
else:
    print("No media_type or interactions column found.")

import os

# Define path
drive_folder = "/content/drive/MyDrive/Spacebar_test"
drive_path = os.path.join(drive_folder, "social_fitness_analysis.csv")

# Create folder if it does not exist
os.makedirs(drive_folder, exist_ok=True)

# Save CSV
social_df.to_csv(drive_path, index=False)
print(f"File saved at: {drive_path}")